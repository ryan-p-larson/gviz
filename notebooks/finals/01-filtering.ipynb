{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "\n",
    "Traverse the tweets and output a filtered subset of the tweets to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "%run utilities.py\n",
    "from multiprocessing import Pool  # faster\n",
    "from time import time as time     # timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def parallelize_series(series, func):\n",
    "    pool = Pool(6)\n",
    "\n",
    "    df = pool.map(func, series)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def filter_tweet_df(in_f):\n",
    "    \"\"\"\n",
    "    Formats and returns a dataframe of tweets\n",
    "    \"\"\"\n",
    "    conversions = {'retweet': lambda x: 1 if (x=='RT') else x,\n",
    "                  'date': lambda t: t.split()[1] +'-'+ t.split()[2] +'-'+ t.split()[-1]\n",
    "                  }\n",
    "    dt_parser = lambda x: pd.to_datetime(x, format=\"%b-%d-%Y\")\n",
    "    \n",
    "    raw_df = pd.read_csv(in_f, usecols=cols_scrape,    # filtering\n",
    "                         low_memory=False, engine='c', # optimization\n",
    "                        converters=conversions,        # format attrs\n",
    "                        nrows=2000)                     # More filtering\n",
    "    \n",
    "    # Drop rows that'll give us trouble\n",
    "    raw_df.dropna(subset=['date', 'longitude', 'latitude'], how='any', inplace=True)\n",
    "    raw_df['date'] = dt_parser(raw_df['date'])\n",
    "    \n",
    "    return raw_df\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    Takes a file, gets all of the names/dirs and filters it. \n",
    "    \"\"\"\n",
    "    raw_f = f.split('/')[-1]\n",
    "    new_f_name = format_new_file(f, 'filtered')\n",
    "    new_f_path = name_file_path(new_f_name, outdir) ## HARD coded directory, just need to rerun\n",
    "    \n",
    "    try:\n",
    "        df = filter_tweet_df(f)\n",
    "        df.to_csv(new_f_path, index=False)\n",
    "        \n",
    "        print (raw_f + ': x')\n",
    "        return {'f': raw_f, 'succeed': True, 'new_f': new_f_path}\n",
    "    except Exception as e:\n",
    "        print (\"Couldn't read: \\t{}\".format(f), e)\n",
    "        return {'f': raw_f, 'succeed': False, 'new_f': new_f_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists, but you can still have the file name\n"
     ]
    }
   ],
   "source": [
    "# Grab all of the files\n",
    "raw_tweets = ls_files_list(external_scrape_dir)\n",
    "\n",
    "# new directory\n",
    "outdir = make_new_dir_date(processed_finals_dir)\n",
    "\n",
    "# combined files in the new dir\n",
    "combined_f = name_file_path('combined.csv', outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run. That. Shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_immigrant_34315.csv: x\n",
      "tweets_immigrant_34375.csv: x\n",
      "tweets_immigrant_34363.csv: x\n",
      "tweets_immigrant_34339.csv: x\n",
      "tweets_immigrant_34316.csv: x\n",
      "tweets_immigrant_34376.csv: x\n",
      "tweets_immigrant_34364.csv: x\n",
      "tweets_immigrant_34351.csv: x\n",
      "tweets_immigrant_34340.csv: x\n",
      "tweets_immigrant_34327.csv: x\n",
      "tweets_immigrant_34317.csv: x\n",
      "tweets_immigrant_34377.csv: x\n",
      "tweets_immigrant_34365.csv: x\n",
      "tweets_immigrant_34341.csv: x\n",
      "tweets_immigrant_34318.csv: x\n",
      "tweets_immigrant_34352.csv: x\n",
      "tweets_immigrant_34378.csv: x\n",
      "tweets_immigrant_34366.csv: x\n",
      "tweets_immigrant_34328.csv: x\n",
      "tweets_immigrant_34342.csv: x\n",
      "tweets_immigrant_34319.csv: x\n",
      "tweets_immigrant_34379.csv: x\n",
      "tweets_immigrant_34367.csv: x\n",
      "tweets_immigrant_34343.csv: x\n",
      "tweets_immigrant_34353.csv: x\n",
      "tweets_immigrant_34320.csv: x\n",
      "tweets_immigrant_34380.csv: x\n",
      "tweets_immigrant_34329.csv: x\n",
      "tweets_immigrant_34368.csv: x\n",
      "tweets_immigrant_34344.csv: x\n",
      "tweets_immigrant_34381.csv: x\n",
      "tweets_immigrant_34369.csv: x\n",
      "tweets_immigrant_34321.csv: x\n",
      "tweets_immigrant_34345.csv: x\n",
      "tweets_immigrant_34354.csv: x\n",
      "tweets_immigrant_34330.csv: x\n",
      "tweets_immigrant_34382.csv: x\n",
      "tweets_immigrant_34370.csv: x\n",
      "tweets_immigrant_34322.csv: x\n",
      "tweets_immigrant_34346.csv: x\n",
      "tweets_immigrant_34383.csv: x\n",
      "tweets_immigrant_34371.csv: x\n",
      "tweets_immigrant_34323.csv: x\n",
      "tweets_immigrant_34355.csv: x\n",
      "tweets_immigrant_34331.csv: x\n",
      "tweets_immigrant_34372.csv: x\n",
      "tweets_immigrant_34384.csv: x\n",
      "tweets_immigrant_34356.csv: x\n",
      "tweets_immigrant_34332.csv: x\n",
      "tweets_immigrant_34373.csv: x\n",
      "tweets_immigrant_34347.csv: x\n",
      "tweets_immigrant_34385.csv: x\n",
      "tweets_immigrant_34324.csv: x\n",
      "tweets_immigrant_34357.csv: x\n",
      "tweets_immigrant_34333.csv: x\n",
      "tweets_immigrant_34374.csv: x\n",
      "tweets_immigrant_34358.csv: x\n",
      "tweets_immigrant_34325.csv: x\n",
      "tweets_immigrant_34334.csv: x\n",
      "tweets_immigrant_34359.csv: x\n",
      "tweets_immigrant_34348.csv: x\n",
      "tweets_immigrant_34386.csv: x\n",
      "tweets_immigrant_34326.csv: x\n",
      "tweets_immigrant_34360.csv: x\n",
      "tweets_immigrant_34387.csv: x\n",
      "tweets_immigrant_34335.csv: x\n",
      "tweets_immigrant_34399.csv: x\n",
      "tweets_immigrant_34361.csv: x\n",
      "tweets_immigrant_34336.csv: x\n",
      "tweets_immigrant_34411.csv: x\n",
      "tweets_immigrant_34362.csv: x\n",
      "tweets_immigrant_34400.csv: x\n",
      "tweets_immigrant_34337.csv: x\n",
      "tweets_immigrant_34388.csv: x\n",
      "tweets_immigrant_34349.csv: x\n",
      "tweets_immigrant_34338.csv: x\n",
      "tweets_immigrant_34412.csv: x\n",
      "tweets_immigrant_34423.csv: x\n",
      "tweets_immigrant_34401.csv: x\n",
      "tweets_immigrant_34435.csv: x\n",
      "tweets_immigrant_34413.csv: x\n",
      "tweets_immigrant_34389.csv: x\n",
      "tweets_immigrant_34350.csv: x\n",
      "tweets_immigrant_34436.csv: x\n",
      "tweets_immigrant_34424.csv: x\n",
      "tweets_immigrant_34414.csv: x\n",
      "tweets_immigrant_34402.csv: x\n",
      "tweets_immigrant_34425.csv: x\n",
      "tweets_immigrant_34390.csv: x\n",
      "tweets_immigrant_34437.csv: x\n",
      "tweets_immigrant_34447.csv: x\n",
      "tweets_immigrant_34415.csv: x\n",
      "tweets_immigrant_34426.csv: x\n",
      "tweets_immigrant_34403.csv: x\n",
      "tweets_immigrant_34391.csv: x\n",
      "tweets_immigrant_34448.csv: x\n",
      "tweets_immigrant_34416.csv: x\n",
      "tweets_immigrant_34404.csv: x\n",
      "tweets_immigrant_34427.csv: x\n",
      "tweets_immigrant_34392.csv: x\n",
      "tweets_immigrant_34438.csv: x\n",
      "tweets_immigrant_34449.csv: x\n",
      "tweets_immigrant_34405.csv: x\n",
      "tweets_immigrant_34417.csv: x\n",
      "tweets_immigrant_34393.csv: x\n",
      "tweets_immigrant_34450.csv: x\n",
      "tweets_immigrant_34428.csv: x\n",
      "tweets_immigrant_34439.csv: x\n",
      "tweets_immigrant_34406.csv: x\n",
      "tweets_immigrant_34418.csv: x\n",
      "tweets_immigrant_34451.csv: x\n",
      "tweets_immigrant_34429.csv: x\n",
      "tweets_immigrant_34394.csv: x\n",
      "tweets_immigrant_34407.csv: x\n",
      "tweets_immigrant_34430.csv: x\n",
      "tweets_immigrant_34440.csv: x\n",
      "tweets_immigrant_34419.csv: x\n",
      "tweets_immigrant_34452.csv: x\n",
      "tweets_immigrant_34408.csv: x\n",
      "tweets_immigrant_34420.csv: x\n",
      "tweets_immigrant_34453.csv: x\n",
      "tweets_immigrant_34441.csv: x\n",
      "tweets_immigrant_34395.csv: x\n",
      "tweets_immigrant_34409.csv: x\n",
      "tweets_immigrant_34454.csv: x\n",
      "tweets_immigrant_34442.csv: x\n",
      "tweets_immigrant_34421.csv: x\n",
      "tweets_immigrant_34431.csv: x\n",
      "tweets_immigrant_34443.csv: x\n",
      "tweets_immigrant_34396.csv: x\n",
      "tweets_immigrant_34432.csv: x\n",
      "tweets_immigrant_34410.csv: x\n",
      "tweets_immigrant_34455.csv: x\n",
      "tweets_immigrant_34444.csv: x\n",
      "tweets_immigrant_34433.csv: x\n",
      "tweets_immigrant_34459.csv: x\n",
      "tweets_immigrant_34456.csv: x\n",
      "tweets_immigrant_34460.csv: x\n",
      "tweets_immigrant_34397.csv: x\n",
      "tweets_immigrant_34422.csv: x\n",
      "tweets_immigrant_34457.csv: x\n",
      "tweets_immigrant_34434.csv: x\n",
      "tweets_immigrant_34461.csv: x\n",
      "tweets_immigrant_34471.csv: x\n",
      "tweets_immigrant_34445.csv: x\n",
      "tweets_immigrant_34483.csv: x\n",
      "tweets_immigrant_34462.csv: x\n",
      "tweets_immigrant_34458.csv: x\n",
      "tweets_immigrant_34398.csv: x\n",
      "tweets_immigrant_34484.csv: x\n",
      "tweets_immigrant_34472.csv: x\n",
      "tweets_immigrant_34463.csv: x\n",
      "tweets_immigrant_34446.csv: x\n",
      "tweets_immigrant_34495.csv: x\n",
      "tweets_immigrant_34507.csv: x\n",
      "tweets_immigrant_34485.csv: x\n",
      "tweets_immigrant_34519.csv: x\n",
      "tweets_immigrant_34496.csv: x\n",
      "tweets_immigrant_34464.csv: x\n",
      "tweets_immigrant_34508.csv: x\n",
      "tweets_immigrant_34473.csv: x\n",
      "tweets_immigrant_34486.csv: x\n",
      "tweets_immigrant_34520.csv: x\n",
      "tweets_immigrant_34497.csv: x\n",
      "tweets_immigrant_34465.csv: x\n",
      "tweets_immigrant_34509.csv: x\n",
      "tweets_immigrant_34487.csv: x\n",
      "tweets_immigrant_34521.csv: x\n",
      "tweets_immigrant_34498.csv: x\n",
      "tweets_immigrant_34466.csv: x\n",
      "tweets_immigrant_34474.csv: x\n",
      "tweets_immigrant_34510.csv: x\n",
      "tweets_immigrant_34488.csv: x\n",
      "tweets_immigrant_34522.csv: x\n",
      "tweets_immigrant_34499.csv: x\n",
      "tweets_immigrant_34467.csv: x\n",
      "tweets_immigrant_34475.csv: x\n",
      "tweets_immigrant_34511.csv: x\n",
      "tweets_immigrant_34489.csv: x\n",
      "tweets_immigrant_34523.csv: x\n",
      "tweets_immigrant_34500.csv: x\n",
      "tweets_immigrant_34468.csv: x\n",
      "tweets_immigrant_34476.csv: x\n",
      "tweets_immigrant_34512.csv: x\n",
      "tweets_immigrant_34490.csv: x\n",
      "tweets_immigrant_34469.csv: x\n",
      "tweets_immigrant_34524.csv: x\n",
      "tweets_immigrant_34501.csv: x\n",
      "tweets_immigrant_34477.csv: x\n",
      "tweets_immigrant_34513.csv: x\n",
      "tweets_immigrant_34491.csv: x\n",
      "tweets_immigrant_34470.csv: x\n",
      "tweets_immigrant_34525.csv: x\n",
      "tweets_immigrant_34502.csv: x\n",
      "tweets_immigrant_34478.csv: x\n",
      "tweets_immigrant_34514.csv: x\n",
      "tweets_immigrant_34492.csv: x\n",
      "tweets_immigrant_34526.csv: x\n",
      "tweets_immigrant_34531.csv: x\n",
      "tweets_immigrant_34503.csv: x\n",
      "tweets_immigrant_34479.csv: x\n",
      "tweets_immigrant_34493.csv: x\n",
      "tweets_immigrant_34515.csv: x\n",
      "tweets_immigrant_34527.csv: x\n",
      "tweets_immigrant_34504.csv: x\n",
      "tweets_immigrant_34480.csv: x\n",
      "tweets_immigrant_34532.csv: x\n",
      "tweets_immigrant_34494.csv: x\n",
      "tweets_immigrant_34516.csv: x\n",
      "tweets_immigrant_34528.csv: x\n",
      "tweets_immigrant_34481.csv: x\n",
      "tweets_immigrant_34533.csv: x\n",
      "tweets_immigrant_34505.csv: x\n",
      "tweets_immigrant_34517.csv: x\n",
      "tweets_immigrant_34543.csv: x\n",
      "tweets_immigrant_34529.csv: x\n",
      "tweets_immigrant_34534.csv: x\n",
      "tweets_immigrant_34482.csv: x\n",
      "tweets_immigrant_34506.csv: x\n",
      "tweets_immigrant_34544.csv: x\n",
      "tweets_immigrant_34518.csv: x\n",
      "tweets_immigrant_34530.csv: x\n",
      "tweets_immigrant_34535.csv: x\n",
      "tweets_immigrant_34555.csv: x\n",
      "tweets_immigrant_34567.csv: x\n",
      "tweets_immigrant_34545.csv: x\n",
      "tweets_immigrant_34536.csv: x\n",
      "tweets_immigrant_34556.csv: x\n",
      "tweets_immigrant_34579.csv: x\n",
      "tweets_immigrant_34568.csv: x\n",
      "tweets_immigrant_34546.csv: x\n",
      "tweets_immigrant_34537.csv: x\n",
      "tweets_immigrant_34557.csv: x\n",
      "tweets_immigrant_34569.csv: x\n",
      "tweets_immigrant_34580.csv: x\n",
      "tweets_immigrant_34547.csv: x\n",
      "tweets_immigrant_34538.csv: x\n",
      "tweets_immigrant_34558.csv: x\n",
      "tweets_immigrant_34581.csv: x\n",
      "tweets_immigrant_34570.csv: x\n",
      "tweets_immigrant_34548.csv: x\n",
      "tweets_immigrant_34539.csv: x\n",
      "tweets_immigrant_34559.csv: x\n",
      "tweets_immigrant_34582.csv: x\n",
      "tweets_immigrant_34571.csv: x\n",
      "tweets_immigrant_34549.csv: x\n",
      "tweets_immigrant_34540.csv: x\n",
      "tweets_immigrant_34560.csv: x\n",
      "tweets_immigrant_34583.csv: x\n",
      "tweets_immigrant_34572.csv: x\n",
      "tweets_immigrant_34550.csv: x\n",
      "tweets_immigrant_34541.csv: x\n",
      "tweets_immigrant_34561.csv: x\n",
      "tweets_immigrant_34584.csv: x\n",
      "tweets_immigrant_34551.csv: x\n",
      "tweets_immigrant_34573.csv: x\n",
      "tweets_immigrant_34542.csv: x\n",
      "tweets_immigrant_34562.csv: x\n",
      "tweets_immigrant_34585.csv: x\n",
      "tweets_immigrant_34552.csv: x\n",
      "tweets_immigrant_34574.csv: x\n",
      "tweets_immigrant_34553.csv: x\n",
      "tweets_immigrant_34563.csv: x\n",
      "tweets_immigrant_34586.csv: x\n",
      "tweets_immigrant_34575.csv: x\n",
      "tweets_immigrant_34554.csv: x\n",
      "tweets_immigrant_34564.csv: x\n",
      "tweets_immigrant_34587.csv: x\n",
      "tweets_immigrant_34576.csv: x\n",
      "tweets_immigrant_34565.csv: x\n",
      "tweets_immigrant_34577.csv: x\n",
      "tweets_immigrant_34566.csv: x\n",
      "tweets_immigrant_34578.csv: x\n",
      "273 / 0 files were filtered.\n",
      "5 files took: 3.3328123092651367 seconds.\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "start = time()  # timer\n",
    "num_files = len(raw_tweets)\n",
    "\n",
    "# execution\n",
    "log_list = parallelize_series(raw_tweets, process_file)\n",
    "log_df = pd.DataFrame(log_list)\n",
    "\n",
    "# clean up\n",
    "end = time()\n",
    "num_converted = log_df[log_df['succeed'] == True].shape[0]\n",
    "\n",
    "# reporting\n",
    "print ('{} / {} files were filtered.'.format(num_converted, (num_files)))\n",
    "print ('{} files took: {} seconds.'.format(5, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>new_f</th>\n",
       "      <th>succeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweets_immigrant_34315.csv</td>\n",
       "      <td>../../data/processed/finals/5-7/tweets_immigra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweets_immigrant_34316.csv</td>\n",
       "      <td>../../data/processed/finals/5-7/tweets_immigra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tweets_immigrant_34317.csv</td>\n",
       "      <td>../../data/processed/finals/5-7/tweets_immigra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tweets_immigrant_34318.csv</td>\n",
       "      <td>../../data/processed/finals/5-7/tweets_immigra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tweets_immigrant_34319.csv</td>\n",
       "      <td>../../data/processed/finals/5-7/tweets_immigra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            f  \\\n",
       "0  tweets_immigrant_34315.csv   \n",
       "1  tweets_immigrant_34316.csv   \n",
       "2  tweets_immigrant_34317.csv   \n",
       "3  tweets_immigrant_34318.csv   \n",
       "4  tweets_immigrant_34319.csv   \n",
       "\n",
       "                                               new_f  succeed  \n",
       "0  ../../data/processed/finals/5-7/tweets_immigra...     True  \n",
       "1  ../../data/processed/finals/5-7/tweets_immigra...     True  \n",
       "2  ../../data/processed/finals/5-7/tweets_immigra...     True  \n",
       "3  ../../data/processed/finals/5-7/tweets_immigra...     True  \n",
       "4  ../../data/processed/finals/5-7/tweets_immigra...     True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_f = name_file_path('log.csv', outdir)\n",
    "log_df.to_csv(log_f, index=False)\n",
    "\n",
    "# take a peek\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab only the succesful files\n",
    "good_fs = log_df[log_df.succeed == True]['new_f'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write it out\n",
    "# first to create headers\n",
    "(pd.read_csv(good_fs[0], low_memory=False, engine='c')).to_csv(combined_f, index=False)\n",
    "\n",
    "with open(combined_f, 'a') as f:\n",
    "    for f_ in good_fs:\n",
    "        # skip the log\n",
    "        if (f_.split('/')[-1] != 'log.csv'):\n",
    "            (pd.read_csv(f_, low_memory=False, engine='c')).to_csv(f, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
