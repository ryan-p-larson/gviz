{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Twitter Scrape\n",
    "\n",
    "Traversing a whole lot of shit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from time import time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# Not Helpful, keeping for (maybe) future\n",
    "#from ldig.predict_tweet import *\n",
    "#detector = Detector('../src/ldig/models/model.latin/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "ext_dir = '../data/external/'\n",
    "proc_dir = '../data/processed/'\n",
    "\n",
    "scrape_in = ext_dir + 'scrape/'\n",
    "scrape_out = proc_dir + 'scrape/3-31/{}-extract.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cols to keep\n",
    "cols_scrape = [\n",
    "    'username',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'date',\n",
    "    'message',\n",
    "    'tweetID',\n",
    "    'language',\n",
    "    'hashtag1',\n",
    "    'hashtag2',\n",
    "    'hashtag3',\n",
    "    'hashtag4',\n",
    "    'hashtag5'\n",
    "]\n",
    "cols_out = [\n",
    "    'username',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'date',\n",
    "    'message',\n",
    "    'hashtag1',\n",
    "    'hashtag2',\n",
    "    'hashtag3',\n",
    "    'hashtag4',\n",
    "    'hashtag5'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "clf = joblib.load('../data/processed/class/3-28/model.pkl')\n",
    "\n",
    "# to prepare us for processing raw tweets\n",
    "%run ../src/twitter-sentiment/preprocessing/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# List of all the files we'll be evaluating\n",
    "\n",
    "# testing\n",
    "#scrape_f = os.listdir(scrape_in)\n",
    "\n",
    "# 'production'\n",
    "scrape_f = [x for x in  os.listdir(scrape_in)\n",
    "           if (x.split('.')[0][-4:] != '_log')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_cleaning(tweet):\n",
    "    try:\n",
    "        return process_one(tweet)\n",
    "    except:\n",
    "        #print (tweet)\n",
    "        return None\n",
    "\n",
    "def process_stemming(tweet):\n",
    "    try:\n",
    "        return stem_one(tweet)\n",
    "    except:\n",
    "        #print (tweet)\n",
    "        return None\n",
    "    \n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, 10)\n",
    "    pool = Pool(4)\n",
    "    #df = pd.concat(pool.map(func, df_split))\n",
    "    df = [pool.map(func, df_split)]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def parallelize_series(series, func):\n",
    "    pool = Pool(6)\n",
    "\n",
    "    df = pool.map(func, series)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_scrape_csv(csv_f, gps=False, verbose=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (verbose):\n",
    "        # Set a pattern for verbosity\n",
    "        metrics = {'f_name': csv_f}\n",
    "        start = time()\n",
    "        print (csv_f)\n",
    "    \n",
    "    # read in\n",
    "    f_ = csv_f\n",
    "    df = pd.read_csv(f_, usecols=cols_scrape, dtype={'message': str})\n",
    "    if (verbose):\n",
    "        metrics['orig'] = df.shape[0]\n",
    "        print (\"Number of tweets\\n\\tOriginal:\\t\\t{}\".format(metrics['orig']))\n",
    "        \n",
    "    # pre screen our messages\n",
    "    df.dropna(subset=['message'], inplace=True)\n",
    "    if (verbose):\n",
    "        metrics['message'] = df.shape[0]\n",
    "        print (\"\\tWith messages:\\t\\t{}\".format(metrics['message']))\n",
    "        \n",
    "    # filter, keep only english tweets\n",
    "    df = df[df.language == 'en']\n",
    "    if (verbose):\n",
    "        metrics['eng'] = df.shape[0]\n",
    "        print (\"\\tLang (twit.) eng.:\\t{}\".format(metrics['eng']))\n",
    "    \n",
    "    # Predict language\n",
    "    ## Tested and it doesn't help...\n",
    "    #df['pred_lang'] = parallelize_series(df.message.values, detector.get_tweet_lang)\n",
    "    #english = df[df.pred_lang == 'en']\n",
    "    #if (verbose):\n",
    "        #print (\"\\tLang (class.) eng.:\\t\\t\\t{}\".format(df.shape[0]))\n",
    "    \n",
    "    # tokenize\n",
    "    df['clean'] = parallelize_series(df.message.values, process_cleaning)\n",
    "    df = df[pd.notnull(df['clean'])]\n",
    "    if (verbose):\n",
    "        metrics['clean'] = df.shape[0]\n",
    "        print (\"\\tWith viable tokens:\\t{}\".format(metrics['clean']))\n",
    "        \n",
    "    # stem\n",
    "    df['stem'] = parallelize_series(df.clean.values, process_stemming)\n",
    "    df = df[pd.notnull(df['stem'])]\n",
    "    if (verbose):\n",
    "        metrics['stem'] = df.shape[0]\n",
    "        print (\"\\tWith stems:\\t\\t{}\".format(metrics['stem']))\n",
    "    \n",
    "    # CLASSIFY\n",
    "    df['label'] = clf.predict(df.stem.values)\n",
    "    df = df[df.label == 1]\n",
    "    if (verbose):\n",
    "        metrics['label'] = df.shape[0]\n",
    "        print (\"\\tClassified+:\\t\\t{}\".format(metrics['label']))\n",
    "        \n",
    "    # Filter to just those with GPS coordinates\n",
    "    if (gps):\n",
    "        df = df[pd.notnull(df['longitude'])]\n",
    "        if (verbose):\n",
    "            metrics['gps'] = df.shape[0]\n",
    "            print ('\\tGPS:\\t\\t\\t')\n",
    "            \n",
    "        \n",
    "    #if verbose, lets log that out\n",
    "    if (verbose):\n",
    "        secs = time() - start\n",
    "        metrics['time'] = secs\n",
    "        print (\"... in {} seconds\\n\".format(secs))\n",
    "        \n",
    "        log.append(metrics)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep a dataframe for a log\n",
    "#log_df = pd.DataFrame(columns=['f_name', 'time', 'orig', 'message', 'eng', 'clean', 'stem', 'label'])\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/external/scrape/tweets_immigrant_34315.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t274900\n",
      "\tWith messages:\t\t274215\n",
      "\tLang (twit.) eng.:\t141850\n",
      "\tWith viable tokens:\t141850\n",
      "\tWith stems:\t\t141849\n",
      "\tClassified+:\t\t1483\n",
      "\tGPS:\t\t\t\n",
      "... in 14.60554552078247 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34316.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t375600\n",
      "\tWith messages:\t\t374678\n",
      "\tLang (twit.) eng.:\t180172\n",
      "\tWith viable tokens:\t180172\n",
      "\tWith stems:\t\t180170\n",
      "\tClassified+:\t\t2341\n",
      "\tGPS:\t\t\t\n",
      "... in 18.990891695022583 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34317.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t358100\n",
      "\tWith messages:\t\t357266\n",
      "\tLang (twit.) eng.:\t172860\n",
      "\tWith viable tokens:\t172860\n",
      "\tWith stems:\t\t172860\n",
      "\tClassified+:\t\t2237\n",
      "\tGPS:\t\t\t\n",
      "... in 18.01806139945984 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34318.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t355200\n",
      "\tWith messages:\t\t354354\n",
      "\tLang (twit.) eng.:\t174685\n",
      "\tWith viable tokens:\t174685\n",
      "\tWith stems:\t\t174681\n",
      "\tClassified+:\t\t1263\n",
      "\tGPS:\t\t\t\n",
      "... in 17.275426387786865 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34319.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t346900\n",
      "\tWith messages:\t\t346303\n",
      "\tLang (twit.) eng.:\t147826\n",
      "\tWith viable tokens:\t147826\n",
      "\tWith stems:\t\t147825\n",
      "\tClassified+:\t\t882\n",
      "\tGPS:\t\t\t\n",
      "... in 15.325823545455933 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34320.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t347591\n",
      "\tWith messages:\t\t346816\n",
      "\tLang (twit.) eng.:\t156660\n",
      "\tWith viable tokens:\t156660\n",
      "\tWith stems:\t\t156660\n",
      "\tClassified+:\t\t2514\n",
      "\tGPS:\t\t\t\n",
      "... in 16.44409942626953 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34321.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t286100\n",
      "\tWith messages:\t\t285484\n",
      "\tLang (twit.) eng.:\t133064\n",
      "\tWith viable tokens:\t133064\n",
      "\tWith stems:\t\t133062\n",
      "\tClassified+:\t\t1407\n",
      "\tGPS:\t\t\t\n",
      "... in 13.884381294250488 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34322.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t30008\n",
      "\tWith messages:\t\t29953\n",
      "\tLang (twit.) eng.:\t12535\n",
      "\tWith viable tokens:\t12535\n",
      "\tWith stems:\t\t12535\n",
      "\tClassified+:\t\t114\n",
      "\tGPS:\t\t\t\n",
      "... in 1.4525120258331299 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34323.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t260500\n",
      "\tWith messages:\t\t259909\n",
      "\tLang (twit.) eng.:\t124851\n",
      "\tWith viable tokens:\t124851\n",
      "\tWith stems:\t\t124851\n",
      "\tClassified+:\t\t1064\n",
      "\tGPS:\t\t\t\n",
      "... in 13.045483350753784 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34324.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t121042\n",
      "\tWith messages:\t\t120717\n",
      "\tLang (twit.) eng.:\t60550\n",
      "\tWith viable tokens:\t60550\n",
      "\tWith stems:\t\t60550\n",
      "\tClassified+:\t\t387\n",
      "\tGPS:\t\t\t\n",
      "... in 6.29401969909668 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34325.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t303200\n",
      "\tWith messages:\t\t302497\n",
      "\tLang (twit.) eng.:\t146700\n",
      "\tWith viable tokens:\t146700\n",
      "\tWith stems:\t\t146699\n",
      "\tClassified+:\t\t1067\n",
      "\tGPS:\t\t\t\n",
      "... in 15.495426654815674 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34326.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t337100\n",
      "\tWith messages:\t\t336204\n",
      "\tLang (twit.) eng.:\t169829\n",
      "\tWith viable tokens:\t169829\n",
      "\tWith stems:\t\t169827\n",
      "\tClassified+:\t\t1137\n",
      "\tGPS:\t\t\t\n",
      "... in 18.044050455093384 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34327.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t351700\n",
      "\tWith messages:\t\t350920\n",
      "\tLang (twit.) eng.:\t162694\n",
      "\tWith viable tokens:\t162694\n",
      "\tWith stems:\t\t162694\n",
      "\tClassified+:\t\t1004\n",
      "\tGPS:\t\t\t\n",
      "... in 16.36262798309326 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34328.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t355400\n",
      "\tWith messages:\t\t354466\n",
      "\tLang (twit.) eng.:\t156718\n",
      "\tWith viable tokens:\t156718\n",
      "\tWith stems:\t\t156717\n",
      "\tClassified+:\t\t1094\n",
      "\tGPS:\t\t\t\n",
      "... in 16.003248929977417 seconds\n",
      "\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34329.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t309100\n",
      "\tWith messages:\t\t308435\n",
      "\tLang (twit.) eng.:\t143076\n",
      "\tWith viable tokens:\t143076\n",
      "Could not read.\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34330.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t302400\n",
      "\tWith messages:\t\t301725\n",
      "\tLang (twit.) eng.:\t139865\n"
     ]
    }
   ],
   "source": [
    "for f in scrape_f:\n",
    "    \n",
    "    # file names\n",
    "    f_in = scrape_in + f\n",
    "    f_out = scrape_out.format(f[:-4])\n",
    "    f_gps = f_out[:-4] + '-gps.csv'\n",
    "    \n",
    "    # sanity check for times sake\n",
    "    if (os.path.isfile(f_in) == False):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            # read in new tweets\n",
    "            labeled_df = process_scrape_csv(f_in, gps=True, verbose=True)\n",
    "            labeled_df.to_csv(f_lat, index=False)\n",
    "            \n",
    "            # write out the now filtered csv\n",
    "            #labeled_df.to_csv(out_f, index=False)\n",
    "            #labeled_df[pd.notnull(labeled_df['longitude'])].to_csv(f_lat, columns=cols_out, index=False)\n",
    "        \n",
    "        except:\n",
    "            log.append({'f_name': f, 'orig': None})\n",
    "            print ('Could not read.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame.from_records(log)\n",
    "log_df.to_csv('../data/processed/scrape/3-31/log.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
