{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Twitter Scrape\n",
    "\n",
    "Traversing a whole lot of shit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from time import time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from ldig.predict_tweet import *\n",
    "detector = Detector('../src/ldig/models/model.latin/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "ext_dir = '../data/external/'\n",
    "proc_dir = '../data/processed/'\n",
    "\n",
    "scrape_in = ext_dir + 'scrape/'\n",
    "scrape_out = proc_dir + 'scrape/{}-extract.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cols to keep\n",
    "cols_scrape = [\n",
    "    'username',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'date',\n",
    "    'message',\n",
    "    'tweetID',\n",
    "    'language',\n",
    "    'hashtag1',\n",
    "    'hashtag2',\n",
    "    'hashtag3',\n",
    "    'hashtag4',\n",
    "    'hashtag5'\n",
    "]\n",
    "cols_out = [\n",
    "    'username',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'date',\n",
    "    'message',\n",
    "    'hashtag1',\n",
    "    'hashtag2',\n",
    "    'hashtag3',\n",
    "    'hashtag4',\n",
    "    'hashtag5'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "clf = joblib.load('../data/processed/class/3-28/model.pkl')\n",
    "\n",
    "# to prepare us for processing raw tweets\n",
    "%run ../src/twitter-sentiment/preprocessing/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# List of all the files we'll be evaluating\n",
    "scrape_f = os.listdir(scrape_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_cleaning(tweet):\n",
    "    try:\n",
    "        return process_one(tweet)\n",
    "    except:\n",
    "        #print (tweet)\n",
    "        return None\n",
    "\n",
    "def process_stemming(tweet):\n",
    "    try:\n",
    "        return stem_one(tweet)\n",
    "    except:\n",
    "        #print (tweet)\n",
    "        return None\n",
    "    \n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, 10)\n",
    "    pool = Pool(4)\n",
    "    #df = pd.concat(pool.map(func, df_split))\n",
    "    df = [pool.map(func, df_split)]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def parallelize_series(series, func):\n",
    "    pool = Pool(6)\n",
    "\n",
    "    df = pool.map(func, series)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_scrape_csv(csv_f, verbose=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (verbose):\n",
    "        # Set a pattern for verbosity\n",
    "        metrics = {'f_name': csv_f}\n",
    "        start = time()\n",
    "        print (csv_f)\n",
    "    \n",
    "    # read in\n",
    "    f_ = csv_f\n",
    "    df = pd.read_csv(f_, usecols=cols_scrape, dtype={'message': str})\n",
    "    if (verbose):\n",
    "        metrics['orig'] = df.shape[0]\n",
    "        print (\"Number of tweets\\n\\tOriginal:\\t\\t{}\".format(metrics['orig']))\n",
    "        \n",
    "    # pre screen our messages\n",
    "    df.dropna(subset=['message'], inplace=True)\n",
    "    if (verbose):\n",
    "        metrics['message'] = df.shape[0]\n",
    "        print (\"\\tWith messages:\\t\\t{}\".format(metrics['message']))\n",
    "        \n",
    "    # filter, keep only english tweets\n",
    "    df = df[df.language == 'en']\n",
    "    if (verbose):\n",
    "        metrics['eng'] = df.shape[0]\n",
    "        print (\"\\tLang (twit.) eng.:\\t{}\".format(metrics['eng']))\n",
    "    \n",
    "    # Predict language\n",
    "    ## Tested and it doesn't help...\n",
    "    #df['pred_lang'] = parallelize_series(df.message.values, detector.get_tweet_lang)\n",
    "    #english = df[df.pred_lang == 'en']\n",
    "    #if (verbose):\n",
    "        #print (\"\\tLang (class.) eng.:\\t\\t\\t{}\".format(df.shape[0]))\n",
    "    \n",
    "    # tokenize\n",
    "    df['clean'] = parallelize_series(df.message.values, process_cleaning)\n",
    "    df = df[pd.notnull(df['clean'])]\n",
    "    if (verbose):\n",
    "        metrics['clean'] = df.shape[0]\n",
    "        print (\"\\tWith viable tokens:\\t{}\".format(metrics['clean']))\n",
    "        \n",
    "    # stem\n",
    "    df['stem'] = parallelize_series(df.clean.values, process_stemming)\n",
    "    df = df[pd.notnull(df['stem'])]\n",
    "    if (verbose):\n",
    "        metrics['stem'] = df.shape[0]\n",
    "        print (\"\\tWith stems:\\t\\t{}\".format(metrics['stem']))\n",
    "    \n",
    "    # CLASSIFY\n",
    "    df['label'] = clf.predict(df.stem.values)\n",
    "    df = df[df.label == 1]\n",
    "    if (verbose):\n",
    "        metrics['label'] = df.shape[0]\n",
    "        print (\"\\tClassified+:\\t\\t{}\".format(metrics['label']))\n",
    "        \n",
    "    #if verbose, lets log that out\n",
    "    if (verbose):\n",
    "        secs = time() - start\n",
    "        metrics['time'] = secs\n",
    "        print (\"... in {} seconds\\n\".format(secs))\n",
    "        \n",
    "        log.append(metrics)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep a dataframe for a log\n",
    "#log_df = pd.DataFrame(columns=['f_name', 'time', 'orig', 'message', 'eng', 'clean', 'stem', 'label'])\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/external/scrape/tweets_immigrant_34316.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t375600\n",
      "\tWith messages:\t\t374678\n",
      "\tLang (twit.) eng.:\t180172\n",
      "\tWith viable tokens:\t180172\n",
      "\tWith stems:\t\t180170\n",
      "\tClassified+:\t\t2341\n",
      "... in 22.337826251983643 seconds\n",
      "\n",
      "../data/external/scrape/tweets_immigrant_34315.csv\n",
      "Number of tweets\n",
      "\tOriginal:\t\t274900\n",
      "\tWith messages:\t\t274215\n",
      "\tLang (twit.) eng.:\t141850\n",
      "\tWith viable tokens:\t141850\n",
      "\tWith stems:\t\t141849\n",
      "\tClassified+:\t\t1483\n",
      "... in 17.7563316822052 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in scrape_f:\n",
    "    \n",
    "    global log_df\n",
    "    # read in new tweets\n",
    "    labeled_df = process_scrape_csv((scrape_in + f), verbose=True)\n",
    "    \n",
    "    # format the new file name, getting rid of '.csv'\n",
    "    out_f = scrape_out.format(f[:-4])\n",
    "    \n",
    "    # write out the now filtered csv\n",
    "    #labeled_df.to_csv(out_f, index=False)\n",
    "    \n",
    "    lat_f = out_f[:-4] + '-gps.csv'\n",
    "    #labeled_df[pd.notnull(labeled_df['longitude'])].to_csv(lat_f, columns=cols_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clean': 180172,\n",
       "  'eng': 180172,\n",
       "  'f_name': '../data/external/scrape/tweets_immigrant_34316.csv',\n",
       "  'label': 2341,\n",
       "  'message': 374678,\n",
       "  'orig': 375600,\n",
       "  'stem': 180170,\n",
       "  'time': 22.337826251983643},\n",
       " {'clean': 141850,\n",
       "  'eng': 141850,\n",
       "  'f_name': '../data/external/scrape/tweets_immigrant_34315.csv',\n",
       "  'label': 1483,\n",
       "  'message': 274215,\n",
       "  'orig': 274900,\n",
       "  'stem': 141849,\n",
       "  'time': 17.7563316822052}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>eng</th>\n",
       "      <th>f_name</th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>orig</th>\n",
       "      <th>stem</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180172</td>\n",
       "      <td>180172</td>\n",
       "      <td>../data/external/scrape/tweets_immigrant_34316...</td>\n",
       "      <td>2341</td>\n",
       "      <td>374678</td>\n",
       "      <td>375600</td>\n",
       "      <td>180170</td>\n",
       "      <td>22.011650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141850</td>\n",
       "      <td>141850</td>\n",
       "      <td>../data/external/scrape/tweets_immigrant_34315...</td>\n",
       "      <td>1483</td>\n",
       "      <td>274215</td>\n",
       "      <td>274900</td>\n",
       "      <td>141849</td>\n",
       "      <td>17.965607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    clean     eng                                             f_name  label  \\\n",
       "0  180172  180172  ../data/external/scrape/tweets_immigrant_34316...   2341   \n",
       "1  141850  141850  ../data/external/scrape/tweets_immigrant_34315...   1483   \n",
       "\n",
       "   message    orig    stem       time  \n",
       "0   374678  375600  180170  22.011650  \n",
       "1   274215  274900  141849  17.965607  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
